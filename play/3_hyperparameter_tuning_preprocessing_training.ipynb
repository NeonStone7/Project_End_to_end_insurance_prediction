{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cfbedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miscellaneous\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob #library that helps us search for files\n",
    "import scipy\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.special import inv_boxcox\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# hyperparameter tuning\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials, partial\n",
    "\n",
    "# preprocessing\n",
    "from category_encoders import OneHotEncoder,TargetEncoder,OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer, PowerTransformer,LabelEncoder, MaxAbsScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict, KFold, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer, PowerTransformer,LabelEncoder, MaxAbsScaler, RobustScaler\n",
    "\n",
    "# models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression,BayesianRidge, ElasticNet, Lasso\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import clone\n",
    "import lightgbm as lgb\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# feature selection / data sampling\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_classif, SelectFromModel, VarianceThreshold\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# saving model\n",
    "import pickle,joblib\n",
    "import boto3\n",
    "\n",
    "# data settings\n",
    "pd.pandas.set_option('display.max_rows',None)\n",
    "pd.pandas.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output = 'pandas')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# custom package\n",
    "import importlib\n",
    "import package\n",
    "importlib.reload(package)\n",
    "\n",
    "from package.data_retrieval.data_retrieval import s3_retrieval, write_to_s3\n",
    "from package.eda.data_exploration import correlation\n",
    "from package.training.model_training import model_evaluation, split_data_by_type, performance_plots\n",
    "from package.preprocessing.data_preprocessing import create_pipeline, winsorize, percentile_imputer, random_sample_imputer, count_encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d3aa4",
   "metadata": {},
   "source": [
    "# Read Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c93522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"C:/Users/Oamen/OneDrive/Documents/DATA PROJECTS/Insurance_claim_prediction_porto/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91365195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open(config_path) as file:\n",
    "    \n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1de137",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cc04135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get access key\n",
    "secret = pd.read_csv(\"C:/Users/Oamen/Downloads/jupyter_iam_accessKeys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f840c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create S3 client object \n",
    "s3_client = boto3.client('s3',aws_access_key_id=secret['Access key ID'][0],aws_secret_access_key=secret['Secret access key'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6d707f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = config['EDA']['bucket_name'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdab1457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object\n",
      "Read bytes\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# retrieve data\n",
    "full_train_df = s3_retrieval(s3_client, bucket_name, config['Model_Selection']['Input_data']['full_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55e39db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_train_df = s3_retrieval(s3_client, bucket_name, reduced_train_new_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c683e2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object\n",
      "Read bytes\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "validation_df = s3_retrieval(s3_client, bucket_name, config['Model_Selection']['Input_data']['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38c3eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object\n",
      "Read bytes\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "test_df = s3_retrieval(s3_client, bucket_name, config['Model_Selection']['Input_data']['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03b3a6",
   "metadata": {},
   "source": [
    "# Training Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f3646",
   "metadata": {},
   "source": [
    "## Preprocessor - Experiment 1\n",
    "\n",
    "1. Missing Values: Simple\n",
    "2. Encoding: One-Hot\n",
    "3. Transformation: Log\n",
    "4. Outliers: Winsorize(0.95-0.05)\n",
    "5. Scaling: StandardScaler\n",
    "6. Resampling: SMOTE\n",
    "7. Feature Selection: Enet Coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ffb1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_exp1, numerical_exp1, xtrain_exp1, ytrain_exp1 = split_data_by_type(full_train_df, config['EDA']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d6d1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_exp1, numerical_exp1, xvalidation_exp1, yvalidation_exp1 = split_data_by_type(validation_df, config['EDA']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44f78b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline_exp1 = Pipeline([\n",
    "    ('transformer', FunctionTransformer(np.log1p)),\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('winsorize', FunctionTransformer(winsorize)),\n",
    "    ('scaler', StandardScaler()),  \n",
    "])\n",
    "\n",
    "cat_pipeline_exp1 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('encoder', OneHotEncoder()),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8af110f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_exp1 = create_pipeline(num_pipeline_exp1, numerical_exp1, cat_pipeline_exp1, categorical_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e04db5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_exp1 = pipeline_exp1.fit_transform(xtrain_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e39e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalidation_exp1 = pipeline_exp1.transform(xvalidation_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e883e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote\n",
    "\n",
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "851df648",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_exp1, ytrain_exp1 = smote.fit_resample(xtrain_exp1, ytrain_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73cdb740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNet(alpha=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(alpha=0.01)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ElasticNet(alpha=0.01)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection - Enet Coefs\n",
    "\n",
    "enet = ElasticNet(l1_ratio = 0.5, alpha = 0.01)\n",
    "\n",
    "enet.fit(xtrain_exp1, ytrain_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b27656a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_columns1 = dict(zip(xtrain_exp1.columns, enet.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3272e936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns_exp1 = [column for column, value in imp_columns1.items() if abs(value) > 0]\n",
    "\n",
    "len(selected_columns_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f610f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_exp1 = xtrain_exp1[selected_columns_exp1]\n",
    "\n",
    "xvalidation_exp1 = xvalidation_exp1[selected_columns_exp1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323a59",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Exp 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac670e6",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d725410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the search space for an over fitting model\n",
    "\n",
    "search_space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 30, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 4, 30, 1),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "    'max_leaf_nodes': hp.quniform('max_leaf_nodes',8,30,1),\n",
    "    'warm_start': hp.choice('warm_start', [True, False]),\n",
    "    'class_weight': hp.choice('class_weight', ['balanced', 'balanced_subsample']),\n",
    "    'ccp_alpha': hp.uniform('ccp_alpha', 0.0, 0.9)\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9ce914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, n_folds, x, y):\n",
    "    \n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_samples_split'] = int(params['min_samples_split'])\n",
    "    params['max_leaf_nodes'] = int(params['max_leaf_nodes'])\n",
    "    \n",
    "\n",
    "    model = RandomForestClassifier(**params, random_state = 0)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv = n_folds, scoring = 'f1', error_score='raise')\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    # to minimize\n",
    "    loss = 1 - max_score\n",
    "    \n",
    "    return {'loss':loss,\n",
    "           'params':params,\n",
    "           'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2b2f81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [4:10:41<00:00, 1880.20s/trial, best loss: 0.22471640506706148]\n"
     ]
    }
   ],
   "source": [
    "# optimize with the TPE algorithm\n",
    "trials_0 = Trials()\n",
    "n_folds = 4\n",
    "\n",
    "best = fmin(fn = partial(objective, n_folds = n_folds,\n",
    "                         x = xtrain_exp1, y = ytrain_exp1),\n",
    "           space = search_space, algo = tpe.suggest, max_evals = 8, trials = trials_0,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "048f88eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.004969665469326723, 'class_weight': 0, 'criterion': 1, 'max_depth': 8.0, 'max_leaf_nodes': 21.0, 'min_samples_split': 12.0, 'min_weight_fraction_leaf': 0.11247527703385163, 'n_estimators': 462.0, 'warm_start': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a328c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22471640506706148\n"
     ]
    }
   ],
   "source": [
    "print(trials_0.best_trial['result']['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d251b8",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1cfc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 2,31,1),\n",
    "    'max_depth': hp.quniform('max_depth', 2,31,1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 2,100,1),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20,100,1),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.01, 0.9),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, 0.9),    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd23b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, n_folds, x, y):\n",
    "    \n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params, random_state = 0)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv = n_folds, scoring = 'f1', error_score='raise')\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    # to minimize\n",
    "    loss = 1 - max_score\n",
    "    \n",
    "    return {'loss':loss,\n",
    "           'params':params,\n",
    "           'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90c8d22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180379 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163430 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176380 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156061 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158318 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166775 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160100 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158495 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174453 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143655 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143290 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223658 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158192 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164167 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134997 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174701 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176621 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157590 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164452 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170872 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137906 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153196 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162772 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148853 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168142 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161548 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212065 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170167 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070339 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7937                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365618, number of negative: 365617                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159914 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7935                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000003                                         \n",
      "[LightGBM] [Info] Start training from score 0.000003                                                                   \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188422 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7938                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "[LightGBM] [Info] Number of positive: 365617, number of negative: 365618                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166496 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7933                                                                                      \n",
      "[LightGBM] [Info] Number of data points in the train set: 731235, number of used features: 32                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499999 -> initscore=-0.000003                                        \n",
      "[LightGBM] [Info] Start training from score -0.000003                                                                  \n",
      "100%|███████████████████████████████████████████████| 8/8 [03:56<00:00, 29.56s/trial, best loss: 1.230804576957123e-05]\n"
     ]
    }
   ],
   "source": [
    "# optimize with the TPE algorithm\n",
    "trials_1 = Trials()\n",
    "n_folds = 4\n",
    "\n",
    "best = fmin(fn = partial(objective, n_folds = n_folds,\n",
    "                         x = xtrain_exp1, y = ytrain_exp1),\n",
    "           space = search_space, algo = tpe.suggest, max_evals = 8, trials = trials_1,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b9241cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 21.0, 'min_child_samples': 60.0, 'n_estimators': 100.0, 'num_leaves': 16.0, 'reg_alpha': 0.8311461646548265, 'reg_lambda': 0.5855522639949706}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5006801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.230804576957123e-05\n"
     ]
    }
   ],
   "source": [
    "print(trials_1.best_trial['result']['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028e3a1",
   "metadata": {},
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1272a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 15, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'gamma': hp.uniform('gamma', 0, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
    "    'scale_pos_weight': hp.quniform('scale_pos_weight', 1, 10, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9383943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, n_folds, x, y):\n",
    "    \n",
    "    params['scale_pos_weight'] = int(params['scale_pos_weight'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params, random_state = 0)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv = n_folds, scoring = 'f1', error_score='raise')\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    # to minimize\n",
    "    loss = 1 - max_score\n",
    "    \n",
    "    return {'loss':loss,\n",
    "           'params':params,\n",
    "           'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae018150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 8/8 [18:33:42<00:00, 8352.85s/trial, best loss: 0.0]\n"
     ]
    }
   ],
   "source": [
    "#  optimize with the TPE algorithm\n",
    "trials_x = Trials()\n",
    "n_folds = 4\n",
    "\n",
    "best = fmin(fn = partial(objective, n_folds = n_folds,\n",
    "                         x = xtrain_exp1, y = ytrain_exp1),\n",
    "           space = search_space, algo = tpe.suggest, max_evals = 8, trials = trials_x,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "164221d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 2.5738810662440015, 'learning_rate': 0.08587333265919023, 'max_depth': 13.0, 'min_child_weight': 3.0, 'n_estimators': 113.0, 'reg_alpha': 0.33463090228747283, 'reg_lambda': 0.1637430846704434, 'scale_pos_weight': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d75b18d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(trials_x.best_trial['result']['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b65ea",
   "metadata": {},
   "source": [
    "## Preprocessor - Experiment 2\n",
    "\n",
    "1. Missing Values: Percentile Imputation\n",
    "2. Encoding: Ordinal\n",
    "3. Transformation: Sqrt\n",
    "4. Outliers: Winsorize(0.95-0.05)\n",
    "5. Scaling: RobustScaler\n",
    "6. Resampling: ADASYN\n",
    "7. Feature Selection: SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c407e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_exp2, numerical_exp2, xtrain_exp2, ytrain_exp2 = split_data_by_type(full_train_df, config['EDA']['target'])\n",
    "\n",
    "categorical_exp2, numerical_exp2, xvalidation_exp2, yvalidation_exp2 = split_data_by_type(validation_df, config['EDA']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "122fa6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline_exp2 = Pipeline([\n",
    "    ('transformer', FunctionTransformer(np.sqrt)),\n",
    "    ('imputer', FunctionTransformer(percentile_imputer)),\n",
    "    ('winsorize', FunctionTransformer(winsorize)),\n",
    "    ('scaler', RobustScaler()),  \n",
    "])\n",
    "\n",
    "cat_pipeline_exp2 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('encoder', OrdinalEncoder()),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bbf3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_exp2 = create_pipeline(num_pipeline_exp2, numerical_exp2, cat_pipeline_exp2, categorical_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b5b69df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_exp2 = pipeline_exp2.fit_transform(xtrain_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "652daabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalidation_exp2 = pipeline_exp2.transform(xvalidation_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11b788b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adasyn\n",
    "\n",
    "adasyn = ADASYN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d418459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_exp2, ytrain_exp2 = adasyn.fit_resample(xtrain_exp2, ytrain_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c263ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "selector = SelectFromModel(rf, threshold=0.0001).fit(xtrain_exp2, ytrain_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53d0551a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns_exp2 = xtrain_exp2.columns[selector.get_support()]\n",
    "\n",
    "len(selected_columns_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2bfab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_exp2 = xtrain_exp2[selected_columns_exp2]\n",
    "\n",
    "xvalidation_exp2 = xvalidation_exp2[selected_columns_exp2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f063e8",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Exp 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33c6ff",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42e24b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the search space for an over fitting model\n",
    "\n",
    "search_space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 500, 1),\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 30, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 4, 30, 1),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "    'max_leaf_nodes': hp.quniform('max_leaf_nodes',8,30,1),\n",
    "    'warm_start': hp.choice('warm_start', [True, False]),\n",
    "    'class_weight': hp.choice('class_weight', ['balanced', 'balanced_subsample']),\n",
    "    'ccp_alpha': hp.uniform('ccp_alpha', 0.0, 0.9)\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8bcca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, n_folds, x, y):\n",
    "    \n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_samples_split'] = int(params['min_samples_split'])\n",
    "    params['max_leaf_nodes'] = int(params['max_leaf_nodes'])\n",
    "    \n",
    "    \n",
    "    model = RandomForestClassifier(**params, random_state = 0)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv = n_folds, scoring = 'f1', error_score='raise')\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    # to minimize\n",
    "    loss = 1 - max_score\n",
    "    \n",
    "    return {'loss':loss,\n",
    "           'params':params,\n",
    "           'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0701b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, n_folds, x, y):\n",
    "    \n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_samples_split'] = int(params['min_samples_split'])\n",
    "    params['max_leaf_nodes'] = int(params['max_leaf_nodes'])\n",
    "    \n",
    "    \n",
    "    model = RandomForestClassifier(**params, random_state = 0)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv = n_folds, scoring = 'f1', error_score='raise')\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    # to minimize\n",
    "    loss = 1 - max_score\n",
    "    \n",
    "    return {'loss':loss,\n",
    "           'params':params,\n",
    "           'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98387e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█████▊                                        | 1/8 [32:39<3:48:34, 1959.18s/trial, best loss: 0.3305379866995021]"
     ]
    }
   ],
   "source": [
    "# optimize with the TPE algorithm\n",
    "trials_2 = Trials()\n",
    "n_folds = 4\n",
    "\n",
    "best = fmin(fn = partial(objective, n_folds = n_folds,\n",
    "                         x = xtrain_exp2, y = ytrain_exp2),\n",
    "           space = search_space, algo = tpe.suggest, max_evals = 8, trials = trials_2,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b59efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trials_2.best_trial['result']['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26118725",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Hyperparameter Tuning'] = {}\n",
    "config['Hyperparameter Tuning']['best_hyperparameters'] = {}\n",
    "\n",
    "\n",
    "lightgbm_best = {'max_depth': 13.0, 'min_child_samples': 45.0, 'n_estimators': 82.0, 'num_leaves': 28.0, 'reg_alpha': 0.8485727275388625, 'reg_lambda': 0.25697353165485826}\n",
    "randomforest_best = {'ccp_alpha': 0.8355154331803291, 'class_weight': 0, 'criterion': 0, 'max_depth': 25.0, 'max_leaf_nodes': 10.0, 'min_samples_split': 16.0, 'min_weight_fraction_leaf': 0.03164012540078487, 'n_estimators': 251.0, 'warm_start': 0}\n",
    "xgboost_best = {'gamma': 1.0704452088125338, 'learning_rate': 0.12998844312823607, 'max_depth': 7.0, 'min_child_weight': 8.0, 'n_estimators': 364.0, 'reg_alpha': 0.053294950056154256, 'reg_lambda': 0.9374649130943011, 'scale_pos_weight': 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50752bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the actual values back to strings\n",
    "to_map_hyperparams = {'criterion': ['gini', 'entropy'],\n",
    "                      'warm_start': [True, False],\n",
    "                      'class_weight': ['balanced', 'balanced_subsample']\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest_best['criterion'] = to_map_hyperparams['criterion'][randomforest_best['criterion']]\n",
    "randomforest_best['warm_start'] = to_map_hyperparams['warm_start'][randomforest_best['warm_start']]\n",
    "randomforest_best['class_weight'] = to_map_hyperparams['class_weight'][randomforest_best['class_weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b723ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Hyperparameter Tuning']['best_hyperparameters']['Random Forest'] = randomforest_best\n",
    "config['Hyperparameter Tuning']['best_hyperparameters']['LightGBM'] = lightgbm_best\n",
    "config['Hyperparameter Tuning']['best_hyperparameters']['Xgboost'] = xgboost_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_hyperparams = ['max_depth', 'min_child_samples', 'n_estimators', 'num_leaves', 'min_samples_split',\n",
    "                  'max_leaf_nodes', 'scale_pos_weight', 'min_child_weight']\n",
    "\n",
    "for hyperparam in int_hyperparams:\n",
    "    \n",
    "    if hyperparam in lightgbm_best:\n",
    "        \n",
    "        lightgbm_best[hyperparam] = int(lightgbm_best[hyperparam])\n",
    "        \n",
    "    if hyperparam in randomforest_best:\n",
    "        \n",
    "        randomforest_best[hyperparam] = int(randomforest_best[hyperparam])        \n",
    "        \n",
    "    if hyperparam in xgboost_best:\n",
    "        \n",
    "        xgboost_best[hyperparam] = int(xgboost_best[hyperparam])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33d1d0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a448a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sets(ytrue, ypred, yproba, set_type, name):\n",
    "\n",
    "    print('Starting evaluation.....')\n",
    "    \n",
    "    \"\"\"This function evaluates the performance of a fitted model. It displays performance plots like confusion_matrix,\n",
    "    roc_curve, precision-recall curve\n",
    "    \n",
    "    Input:\n",
    "    ytrue: true values of y\n",
    "    ypred: predicted values of y\n",
    "    yproba: predicted probabilities of y\n",
    "    \n",
    "    Outputs: \n",
    "    DataFrame with models and metrics, and plots\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy, precision, recall, f1_scores, auc_scores = [],[],[],[],[]\n",
    "            \n",
    "    acc = accuracy_score(ytrue, ypred)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "    prec =  precision_score(ytrue, ypred)\n",
    "    precision.append(prec)\n",
    "\n",
    "    rec =  recall_score(ytrue, ypred)\n",
    "    recall.append(rec)\n",
    "\n",
    "    f1_ =  f1_score(ytrue, ypred)\n",
    "    f1_scores.append(f1_)\n",
    "\n",
    "    auc =  roc_auc_score(ytrue, ypred)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(ytrue, yproba)\n",
    "\n",
    "    precisions, recalls, _ =  precision_recall_curve(ytrue, yproba)\n",
    "\n",
    "    cm = confusion_matrix(ytrue, ypred)\n",
    "\n",
    "    performance_plots(cm,fpr,tpr,auc,name, precisions,recalls)\n",
    "\n",
    "    print(classification_report(ytrue, ypred))\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame({\n",
    "    'Set type': set_type,\n",
    "    'AUC': auc_scores,\n",
    "    'F1': f1_scores,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall\n",
    "    }, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Model Training'] = {}\n",
    "\n",
    "config['Model Training']['Preprocessor'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1de54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into dependent and independent and by type\n",
    "\n",
    "categorical_col_input, numerical_col_input, xtrain, ytrain = split_data_by_type(full_train_df, config['EDA']['target'])\n",
    "\n",
    "_, _, xtest, ytest = split_data_by_type(test_df, config['EDA']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Model Training']['Preprocessor']['Categorical_input_columns'] = categorical_col_input\n",
    "\n",
    "config['Model Training']['Preprocessor']['Numerical_input_columns'] = numerical_col_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('transformer', FunctionTransformer(np.sqrt)),\n",
    "    ('imputer', FunctionTransformer(percentile_imputer)),\n",
    "    ('winsorize', FunctionTransformer(winsorize)),\n",
    "    ('scaler', RobustScaler()),  \n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('encoder', OrdinalEncoder()),\n",
    "    \n",
    "])\n",
    "\n",
    "main_pipeline = create_pipeline(num_pipeline, numerical_col_input, cat_pipeline, categorical_col_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea176a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipeline.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "\n",
    "xtrain_preprocessed = main_pipeline.transform(xtrain)\n",
    "\n",
    "xtest_preprocessed = main_pipeline.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d301d",
   "metadata": {},
   "source": [
    "## Training run 1 - Trying to combat overfitting the minority\n",
    "\n",
    "- Using Ensembling and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fcd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample training data\n",
    "\n",
    "undersampler = RandomUnderSampler()\n",
    "\n",
    "xtrain_samp, ytrain_samp = undersampler.fit_resample(xtrain_preprocessed, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3af3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_samp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "selector = SelectFromModel(rf, threshold=0.01).fit(xtrain_samp, ytrain_samp)\n",
    "\n",
    "selected_columns_1 = xtrain.columns[selector.get_support()]\n",
    "\n",
    "len(selected_columns_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f955387",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_samp_1 = xtrain_samp[selected_columns_1]\n",
    "\n",
    "xtest_preprocessed_1 = xtest_preprocessed[selected_columns_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e46d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Model Training']['Preprocessor']['Important_columns'] = {}\n",
    "\n",
    "config['Model Training']['Preprocessor']['Important_columns']['Run_1'] = selected_columns_1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c224b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(**xgboost_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af3a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "final_model.fit(xtrain_samp_1, ytrain_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f3b46",
   "metadata": {},
   "source": [
    "## Precision-Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ca344",
   "metadata": {},
   "outputs": [],
   "source": [
    "yscores = cross_val_predict(final_model, xtrain_samp_1, ytrain_samp, cv = 3, method = 'predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78512557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(ytrain_samp, yscores[:, 1])\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], label = 'Precision')\n",
    "plt.plot(thresholds, recalls[:-1], label = 'Recall')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c64605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f'Optimal Threshold: {optimal_threshold}')\n",
    "print(f'Precision: {precisions[optimal_idx]}')\n",
    "print(f'Recall: {recalls[optimal_idx]}')\n",
    "print(f'F1 Score: {f1_scores[optimal_idx]}')\n",
    "\n",
    "# Apply the optimal threshold to convert probabilities to class labels\n",
    "y_pred = (yscores[:, 1] >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yscores = final_model.predict_proba(xtest_preprocessed_1)[:, 1]\n",
    "\n",
    "ypred_80 = (test_yscores >= optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29988556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred_80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6e70c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196e6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = final_model.predict(xtrain_samp_1)\n",
    "\n",
    "y_train_proba = final_model.predict_proba(xtrain_samp_1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a5930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results = evaluate_sets(ytrain_samp, y_train_pred, y_train_proba, \n",
    "                              'Train', 'Voting Ensemble Train')\n",
    "\n",
    "train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539afee4",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce371f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = final_model.predict(xtest_preprocessed_1)\n",
    "\n",
    "y_test_proba = final_model.predict_proba(xtest_preprocessed_1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6d217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results = evaluate_sets(ytest, y_test_pred, y_test_proba, \n",
    "                             'Test', 'Voting Ensemble Test')\n",
    "\n",
    "test_results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d93344",
   "metadata": {},
   "source": [
    "## Training run 2 - Trying to combat overfitting the minority\n",
    "\n",
    "- Using Ensembling and Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample training data\n",
    "\n",
    "smote = BorderlineSMOTE()\n",
    "\n",
    "xtrain_samp, ytrain_samp = smote.fit_resample(xtrain_preprocessed, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34be237",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'l1_ratio': np.linspace(0.5, 1.0, 10).tolist(),\n",
    "             'alpha': np.linspace(0.01, 1.0, 20).tolist()\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908cb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic net coef\n",
    "\n",
    "enet = ElasticNet(l1_ratio = 0.5, alpha = 0.01)\n",
    "\n",
    "grid = GridSearchCV(enet, param_grid, cv = StratifiedKFold(n_splits = 3), scoring = 'f1',refit = True)\n",
    "grid.fit(xtrain_samp, ytrain_samp)\n",
    "\n",
    "selected_cols_2 = grid.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a572a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_coefs = dict(zip(xtrain_samp.columns, selected_cols_2))\n",
    "\n",
    "Important_columns = [column for column, value in enet_coefs.items() if abs(value) > 0]\n",
    "\n",
    "len(Important_columns), len(xtrain_samp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66780d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_samp = xtrain_samp[Important_columns]\n",
    "\n",
    "xtest_preprocessed_2 = xtest_preprocessed[Important_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Model Training']['Preprocessor']['Important_columns']['Run_2'] = Important_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b01494",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = VotingClassifier([\n",
    "    ('forest', RandomForestClassifier(**randomforest_best)),\n",
    "    ('lgb', lgb.LGBMClassifier(**lightgbm_best)),\n",
    "    ('xgb', xgb.XGBClassifier(**xgboost_best)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "final_model.fit(xtrain_samp, ytrain_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52d070",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = final_model.predict(xtrain_samp)\n",
    "\n",
    "y_train_proba = final_model.predict_proba(xtrain_samp)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c56978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results = evaluate_sets(ytrain_samp, y_train_pred, y_train_proba, \n",
    "                              'Train', 'Voting Ensemble Train')\n",
    "\n",
    "train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e5413",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e44f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = final_model.predict(xtest_preprocessed_2)\n",
    "\n",
    "y_test_proba = final_model.predict_proba(xtest_preprocessed_2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c4030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results = evaluate_sets(ytest, y_test_pred, y_test_proba, \n",
    "                             'Test', 'Voting Ensemble Test')\n",
    "\n",
    "test_results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04a5e4",
   "metadata": {},
   "source": [
    "# Save to config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79658f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Serializing json\n",
    "# json_object = json.dumps(config, indent=4)\n",
    " \n",
    "# # Writing to sample.json\n",
    "# with open(\"C:/Users/Oamen/OneDrive/Documents/DATA PROJECTS/Insurance_claim_prediction_porto/config.json\", \"w\") as outfile:\n",
    "#     outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
